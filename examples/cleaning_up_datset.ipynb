{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/dansbecker/handling-missing-values/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_missing_values = original_data.dropna(axis=1)  # drop column w missing val    \n",
    "\n",
    "cols_with_missing = [col for col in original_data.columns if original_data[col].isnull().any()]\n",
    "reduced_original_data = original_data.drop(cols_with_missing, axis=1)\n",
    "reduced_test_data = test_data.drop(cols_with_missing, axis=1)  #  test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classic way finding out missing vals\n",
    "missing_val_count_by_column = (data.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "my_imputer = SimpleImputer()\n",
    "data_with_imputed_values = my_imputer.fit_transform(original_data)  # imputes w mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make copy to avoid changing original data (when Imputing)\n",
    "new_data = original_data.copy()\n",
    "\n",
    "# make new columns indicating what will be imputed\n",
    "cols_with_missing = (col for col in new_data.columns \n",
    "                                 if new_data[col].isnull().any())\n",
    "for col in cols_with_missing:\n",
    "    new_data[col + '_was_missing'] = new_data[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "new_data = pd.DataFrame(my_imputer.fit_transform(new_data))\n",
    "new_data.columns = original_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melburn price prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "melb_data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "melb_target = melb_data.Price\n",
    "melb_predictors = melb_data.drop(['Price'], axis=1)\n",
    "\n",
    "# For the sake of keeping the example simple, we'll use only numeric predictors. \n",
    "melb_numeric_predictors = melb_predictors.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've loaded a function score_dataset(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(melb_numeric_predictors, \n",
    "                                                    melb_target,\n",
    "                                                    train_size=0.7, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0)\n",
    "#   function calculating mean_absolute_error\n",
    "def score_dataset(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return mean_absolute_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Model Score from Dropping Columns with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_test  = X_test.drop(cols_with_missing, axis=1)\n",
    "print(\"Mean Absolute Error from dropping columns with Missing Values:\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Model Score from Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train = my_imputer.fit_transform(X_train)\n",
    "imputed_X_test = my_imputer.transform(X_test)\n",
    "print(\"Mean Absolute Error from Imputation:\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Score from Imputation with Extra Columns Showing What Was Imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_X_train_plus = X_train.copy()\n",
    "imputed_X_test_plus = X_test.copy()\n",
    "​\n",
    "cols_with_missing = (col for col in X_train.columns \n",
    "                                 if X_train[col].isnull().any())\n",
    "for col in cols_with_missing:\n",
    "    imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n",
    "    imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n",
    "​\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = my_imputer.fit_transform(imputed_X_train_plus)\n",
    "imputed_X_test_plus = my_imputer.transform(imputed_X_test_plus)\n",
    "​\n",
    "print(\"Mean Absolute Error from Imputation while Track What Was Imputed:\")\n",
    "print(score_dataset(imputed_X_train_plus, imputed_X_test_plus, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple treatment of missing vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules we'll use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read in all our data\n",
    "nfl_data = pd.read_csv(\"../input/nflplaybyplay2009to2016/NFL Play by Play 2009-2017 (v4).csv\")\n",
    "sf_permits = pd.read_csv(\"../input/building-permit-applications-data/Building_Permits.csv\")\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Columns with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_val_count_by_column = (data.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_missing = [col for col in original_data.columns \n",
    "                                 if original_data[col].isnull().any()]\n",
    "reduced_original_data = original_data.drop(cols_with_missing, axis=1)\n",
    "reduced_test_data = test_data.drop(cols_with_missing, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "my_imputer = SimpleImputer()\n",
    "data_with_imputed_values = my_imputer.fit_transform(original_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating, Reading, Writing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('max_rows', 5)\n",
    "from learntools.core import binder; binder.bind(globals())\n",
    "from learntools.pandas.creating_reading_and_writing import *\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here. Create a dataframe matching the above diagram and assign it to the variable fruits.\n",
    "fruits = pd.DataFrame({'Apples':[30], 'Bananas': [21]})  #  or\n",
    "fruits = pd.DataFrame([ [30, 21]], columns=['Apples', 'Bananas'])\n",
    "fruit_sales = pd.DataFrame([[35, 21], [41, 34]], columns=['Apples', 'Bananas'], index=['2017 Sales', '2018 Sales'])\n",
    "# Check your answer\n",
    "q1.check()\n",
    "fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = ['4 cups', '1 cup', '2 large', '1 can']\n",
    "i = ['Flour', 'Milk', 'Eggs', 'Spam']\n",
    "ingredients = pd.Series(q, index=i,name='Dinner')\n",
    "# Check your answer\n",
    "q3.check()\n",
    "ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('../input/wine-reviews/winemag-data_first150k.csv', index_col=0)\n",
    "animals = pd.DataFrame({'Cows': [12, 20], 'Goats': [22, 19]}, index=['Year 1', 'Year 2'])\n",
    "animals\n",
    "animals.to_csv(\"cows_and_goats.csv\")     # to save the frame as csv\n",
    "reviews.set_index(\"title\")  # to set index put a column or list [] with same len\n",
    "reviews.country == 'Italy'  # conditional returning False or True column\n",
    "reviews.loc[reviews.country == 'Italy']   # then use the conditional to select based on it\n",
    "reviews.loc[(reviews.country == 'Italy') & (reviews.points >= 90)]  # combine 2 conditionals\n",
    "reviews.loc[(reviews.country == 'Italy') | (reviews.points >= 90)]\n",
    "reviews.loc[reviews.country.isin(['Italy', 'France'])]   \n",
    "reviews.loc[reviews.price.notnull()]  #  to select values which are not null\n",
    "reviews['critic'] = 'everyone'               #  asign constant val   \n",
    "reviews['index_backwards'] = range(len(reviews), 0, -1)   #   ... or more vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [1, 2, 3, 5, 8]  # select few records \n",
    "sample_reviews = reviews.loc[indices]  #  or\n",
    "sample_reviews = reviews.iloc[[1,2,3,5,8]]   \n",
    "\n",
    "cols = ['country', 'province', 'region_1',  'region_2']  # create df from few indices and columns\n",
    "indices = [0, 1, 10,  100]\n",
    "df = reviews.loc[indices, cols]\n",
    "\n",
    "cols2 = ['country', 'variety']\n",
    "df = reviews.loc[:99, cols2]   \n",
    "\n",
    "cols = ['country', 'variety']\n",
    "df = reviews.loc[:99, cols]\n",
    "\n",
    "cols_idx = [0, 11]\n",
    "df = reviews.iloc[:100, cols_idx]\n",
    "\n",
    "italian_wines = reviews[reviews.country == 'Italy']\n",
    "top_oceania_wines = reviews.loc[(reviews.country.isin(['Australia', 'New Zealand'] )) & (reviews.points >= 95)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_points = reviews.points.median()\n",
    "countries = reviews.country.unique()\n",
    "reviews_per_country = reviews.country.value_counts()\n",
    "centered_price = reviews.price - reviews.price.mean()\n",
    "# CHOOSE BEST WINE BY POINTS TO PRICE RATIO\n",
    "bargain_idx = (reviews.points / reviews.price).idxmax()\n",
    "bargain_wine = reviews.loc[bargain_idx, 'title']\n",
    " # COUNT WORDS IN DESCRIPTION\n",
    "n_trop = reviews.description.map(lambda desc: \"tropical\" in desc).sum()\n",
    "n_fruity = reviews.description.map(lambda desc: \"fruity\" in desc).sum()\n",
    "descriptor_counts = pd.Series([n_trop, n_fruity], index=['tropical', 'fruity'])\n",
    "#remap ratings in stars from points\n",
    "def stars(row):\n",
    "    if row.country == 'Canada':\n",
    "        return 3\n",
    "    elif row.points >= 95:\n",
    "        return 3\n",
    "    elif row.points >= 85:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "star_ratings = reviews.apply(stars, axis='columns')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregated or Descriptive funcs in pd and  MAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.points.describe()\n",
    "reviews.points.mean()\n",
    "reviews.taster_name.unique()\n",
    "reviews.taster_name.value_counts()\n",
    "\n",
    "review_points_mean = reviews.points.mean()\n",
    "reviews.points.map(lambda p: p - review_points_mean)    #   MAP mean to pints\n",
    "#   or to treat the entire DF\n",
    "def remean_points(row):\n",
    "    row.points = row.points - review_points_mean\n",
    "    return row\n",
    "\n",
    "reviews.apply(remean_points, axis='columns')\n",
    "#   or\n",
    "review_points_mean = reviews.points.mean()\n",
    "reviews.points - review_points_mean\n",
    "# combine of equal len t series \n",
    "reviews.country + \" --- \" + reviews.region_1\n",
    "\n",
    "reviews_written = reviews.groupby('taster_twitter_handle').size()\n",
    "#   or\n",
    "reviews_written = reviews.groupby('taster_twitter_handle').taster_twitter_handle.count()\n",
    "best_rating_per_price = reviews.groupby('price')['points'].max().sort_index()  # best rating per price\n",
    "\n",
    "price_extremes = reviews.groupby('variety').price.agg([min, max])\n",
    "sorted_varieties = price_extremes.sort_values(by=['min', 'max'], ascending=False)\n",
    "reviewer_mean_ratings = reviews.groupby('taster_name').points.mean()  # Create a Series whose index is reviewers and whose values is the average review score given out by that reviewer\n",
    "country_variety_counts = reviews.groupby(['country', 'variety']).size().sort_values(ascending=False)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.groupby('points').points.count()\n",
    "reviews.groupby('points').price.min()\n",
    "reviews.groupby(['country', 'province']).apply(lambda df: df.loc[df.points.idxmax()])  \n",
    "reviews.groupby(['country']).price.agg([len, min, max])\n",
    "# MULTI INDEX\n",
    "countries_reviewed = reviews.groupby(['country', 'province']).description.agg([len])\n",
    "countries_reviewed\n",
    "countries_reviewed.reset_index() \n",
    "countries_reviewed.sort_values(by='len')\n",
    "countries_reviewed.sort_values(by='len', ascending=False)\n",
    "countries_reviewed.sort_index()\n",
    "countries_reviewed.sort_values(by=['country', 'len'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dtypes  and missing vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.dtypes  #  country        object     description    object   points   object\n",
    "reviews.points.astype('float64')\n",
    "\n",
    "reviews[pd.isnull(reviews.country)]\n",
    "\n",
    "reviews.region_2.fillna(\"Unknown\")\n",
    "\n",
    "reviews.taster_twitter_handle.replace(\"@kerinokeefe\", \"@kerino\")\n",
    "dtype = reviews.points.dtypes\n",
    "point_strings = reviews.points.astype(str)  # nums to str \n",
    "# find missing vals\n",
    "n_missing_prices = reviews.price.isnull().sum()   \n",
    "# or\n",
    "n_missing_prices = pd.isnull(reviews.price).sum()\n",
    "reviews_per_region = reviews.region_1.fillna('Unknown').value_counts().sort_values(ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.rename(columns={'points': 'score'})\n",
    "reviews.rename(index={0: 'firstEntry', 1: 'secondEntry'})\n",
    "reviews.rename_axis(\"wines\", axis='rows').rename_axis(\"fields\", axis='columns')\n",
    "\n",
    "renamed = reviews.rename(columns=dict(region_1='region', region_2='locale'))\n",
    "canadian_youtube = pd.read_csv(\"../input/youtube-new/CAvideos.csv\")\n",
    "british_youtube = pd.read_csv(\"../input/youtube-new/GBvideos.csv\")\n",
    "\n",
    "pd.concat([canadian_youtube, british_youtube])\n",
    "\n",
    "left = canadian_youtube.set_index(['title', 'trending_date'])\n",
    "right = british_youtube.set_index(['title', 'trending_date'])\n",
    "\n",
    "left.join(right, lsuffix='_CAN', rsuffix='_UK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale and Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules we'll use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for Box-Cox Transformation\n",
    "from scipy import stats\n",
    "\n",
    "# for min_max scaling\n",
    "from mlxtend.preprocessing import minmax_scaling\n",
    "\n",
    "# plotting modules\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read in all our data\n",
    "kickstarters_2017 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201801.csv\")\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 1000 data points randomly drawn from an exponential distribution\n",
    "original_data = np.random.exponential(size = 1000)\n",
    "\n",
    "# mix-max scale the data between 0 and 1\n",
    "scaled_data = minmax_scaling(original_data, columns = [0])\n",
    "\n",
    "# plot both together to compare\n",
    "fig, ax=plt.subplots(1,2)\n",
    "sns.distplot(original_data, ax=ax[0])\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.distplot(scaled_data, ax=ax[1])\n",
    "ax[1].set_title(\"Scaled data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the exponential data with boxcox\n",
    "normalized_data = stats.boxcox(original_data)\n",
    "\n",
    "# plot both together to compare\n",
    "fig, ax=plt.subplots(1,2)\n",
    "sns.distplot(original_data, ax=ax[0])\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.distplot(normalized_data[0], ax=ax[1])\n",
    "ax[1].set_title(\"Normalized data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the usd_goal_real column\n",
    "usd_goal = kickstarters_2017.usd_goal_real\n",
    "\n",
    "# scale the goals from 0 to 1\n",
    "scaled_data = minmax_scaling(usd_goal, columns = [0])\n",
    "\n",
    "# plot the original & scaled data together to compare\n",
    "fig, ax=plt.subplots(1,2)\n",
    "sns.distplot(usd_goal, ax=ax[0])\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.distplot(scaled_data, ax=ax[1])\n",
    "ax[1].set_title(\"Scaled data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index of all positive pledges (Box-Cox only takes postive values)\n",
    "index_of_positive_pledges = kickstarters_2017.usd_pledged_real > 0\n",
    "\n",
    "# get only positive pledges (using their indexes)\n",
    "positive_pledges = kickstarters_2017.usd_pledged_real.loc[index_of_positive_pledges]\n",
    "\n",
    "# normalize the pledges (w/ Box-Cox)\n",
    "normalized_pledges = stats.boxcox(positive_pledges)[0]\n",
    "\n",
    "# plot both together to compare\n",
    "fig, ax=plt.subplots(1,2)\n",
    "sns.distplot(positive_pledges, ax=ax[0])\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.distplot(normalized_pledges, ax=ax[1])\n",
    "ax[1].set_title(\"Normalized data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules we'll use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "# read in our data\n",
    "earthquakes = pd.read_csv(\"../input/earthquake-database/database.csv\")\n",
    "landslides = pd.read_csv(\"../input/landslide-events/catalog.csv\")\n",
    "volcanos = pd.read_csv(\"../input/volcanic-eruptions/database.csv\")\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landslides['date_parsed'] = pd.to_datetime(landslides['date'], format = \"%m/%d/%y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landslides['date_parsed'] = pd.to_datetime(landslides['Date'], infer_datetime_format=True)  # for harder cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_of_month_landslides = landslides['date_parsed'].dt.day  # to get the day (convert to dt from object first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove na's\n",
    "day_of_month_landslides = day_of_month_landslides.dropna()\n",
    "\n",
    "# plot the day of the month\n",
    "sns.distplot(day_of_month_landslides, kde=False, bins=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_of_month_earthquake = earthquakes['date_parsed'].dt.day  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules we'll use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# helpful character encoding module\n",
    "import chardet\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with a string\n",
    "before = \"This is the euro symbol: €\"\n",
    "\n",
    "# check to see what datatype it is\n",
    "type(before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode it to a different encoding, replacing characters that raise errors\n",
    "after = before.encode(\"utf-8\", errors = \"replace\")\n",
    "\n",
    "# check the type\n",
    "type(after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it back to utf-8\n",
    "print(after.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inconsistent Data Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules we'll use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# helpful modules\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import process\n",
    "import chardet\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first ten thousand bytes to guess the character encoding\n",
    "with open(\"../input/PakistanSuicideAttacks Ver 11 (30-November-2017).csv\", 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(100000))\n",
    "\n",
    "# check what the character encoding might be\n",
    "print(result)    #  {'encoding': 'Windows-1252', 'confidence': 0.73, 'language': ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the unique values in the 'City' column\n",
    "cities = suicide_attacks['City'].unique()\n",
    "\n",
    "# sort them alphabetically and then take a closer look\n",
    "cities.sort()\n",
    "cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case\n",
    "suicide_attacks['City'] = suicide_attacks['City'].str.lower()\n",
    "# remove trailing white spaces\n",
    "suicide_attacks['City'] = suicide_attacks['City'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 10 closest matches to \"d.i khan\"   extract vals that dont fit\n",
    "matches = fuzzywuzzy.process.extract(\"d.i khan\", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "\n",
    "# take a look at them\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to replace rows in the provided column of the provided dataframe\n",
    "# that match the provided string above the provided ratio with the provided string\n",
    "def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):\n",
    "    # get a list of unique strings\n",
    "    strings = df[column].unique()\n",
    "    \n",
    "    # get the top 10 closest matches to our input string\n",
    "    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n",
    "                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "\n",
    "    # only get matches with a ratio > 90\n",
    "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n",
    "\n",
    "    # get the rows of all the close matches in our dataframe\n",
    "    rows_with_matches = df[column].isin(close_matches)\n",
    "\n",
    "    # replace all rows with close matches with the input matches \n",
    "    df.loc[rows_with_matches, column] = string_to_match\n",
    "    \n",
    "    # let us know the function's done\n",
    "    print(\"All done!\")\n",
    "    \n",
    "    # use the function we just wrote to replace close matches to \"d.i khan\" with \"d.i khan\"\n",
    "replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=\"d.i khan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning tutorial from kagel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up code checking\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.machine_learning.ex2 import *\n",
    "print(\"Setup Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path of the file to read\n",
    "iowa_file_path = '../input/home-data-for-ml-course/train.csv'\n",
    "\n",
    "# Fill in the line below to read the file into a variable home_data\n",
    "home_data = pd.read_csv(iowa_file_path)\n",
    "\n",
    "# Call line below with no argument to check that you've loaded the data correctly\n",
    "step_1.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'\n",
    "melbourne_data = pd.read_csv(melbourne_file_path) \n",
    "melbourne_data.columns\n",
    "melbourne_data = melbourne_data.dropna(axis=0)\n",
    "y = melbourne_data.Price\n",
    "melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']\n",
    "X = melbourne_data[melbourne_features]\n",
    "X.describe()\n",
    "X.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define: \n",
    "What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too.\n",
    "#### Fit: \n",
    "Capture patterns from provided data. This is the heart of modeling.\n",
    "##### Predict: \n",
    "Just what it sounds like\n",
    "#### Evaluate: \n",
    "Determine how accurate the model's predictions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Define model. Specify a number for random_state to ensure same results each run\n",
    "melbourne_model = DecisionTreeRegressor(random_state=1)\n",
    "\n",
    "# Fit model\n",
    "melbourne_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Making predictions for the following 5 houses:\")\n",
    "print(X.head())\n",
    "print(\"The predictions are\")\n",
    "print(melbourne_model.predict(X.head()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2rc1 ('pandas-profiling-hEhKKCs7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2rc1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e515a9a07699049b9e16d47a09f6f903ae79dbd127f4bc5c5a25fcae9dd07596"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
